{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Immigration Cases in the US\n",
    "### Data Engineering Capstone Project\n",
    "\n",
    "#### Project Summary\n",
    "In this project, data from various sources will be utilized for further processing in order to enable fast and efficient access to them. The data includes information about immigration cases in the US. \n",
    "\n",
    "\n",
    "The project follows the follow steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Do all imports and installs here\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Create spark session\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.\\\n",
    "config(\"spark.jars.repositories\", \"https://repos.spark-packages.org/\").\\\n",
    "config(\"spark.jars.packages\", \"saurfang:spark-sas7bdat:2.0.0-s_2.11\").\\\n",
    "enableHiveSupport().getOrCreate()\n",
    "\n",
    "df_spark = spark.read.format('com.github.saurfang.sas.spark').load('../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 1: Scope the Project and Gather Data\n",
    "\n",
    "#### Scope \n",
    "Explain what you plan to do in the project in more detail. What data do you use? What is your end solution look like? What tools did you use? \n",
    "\n",
    "* In this project, data from the sources mentioned underneath is used to create a data pipeline and database that uses a star schema. Therefore the data will be analyzed to find data quality issues. Afterwards the data will be cleaned to create a fact table and multiple dimension tables then. In the end data quality checks are performed.\n",
    "\n",
    "* The project uses the framework Spark to extract and transform the data, to create tables and write them to parquet files.\n",
    "\n",
    "#### Describe and Gather Data \n",
    "Describe the data sets you're using. Where did it come from? What type of information is included?\n",
    "\n",
    "The following data sets are used in this project:\n",
    "* [I94 Immigration Data](https://www.trade.gov/national-travel-and-tourism-office): The data set comes from the US National Tourism and Trade Office and includes information about immigrants in the United States.  \n",
    "* [World Temperature Data](https://www.kaggle.com/berkeleyearth/climate-change-earth-surface-temperature-data): The data set comes from Kaggle and includes data from the Berkeley Earth Surface Temperature Study. It includes historical data about the temperature in over 3000 cities worldwide.\n",
    "* [U.S. City Demographic Data](https://public.opendatasoft.com/explore/dataset/us-cities-demographics/table/): This data set comes from opendatasoft and consists of different information regarding the demographic data of various US cities. \n",
    "* [Airport Code Table](https://datahub.io/core/airport-codes#data): The data set comes from datahub.io and includes the IATA/ICAO airport codes from airports around the world. \n",
    "* Additionally the information from the I94 Immigration Label file is seperated into two csv files:\n",
    " * The country codes - get a country by the number mentioned in the immigration data\n",
    " * The airport codes that are used in the I94port field do not appear in the airport data, a mapping is given in this file "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### Temperature data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- dt: string (nullable = true)\n",
      " |-- AverageTemperature: string (nullable = true)\n",
      " |-- AverageTemperatureUncertainty: string (nullable = true)\n",
      " |-- City: string (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      " |-- Latitude: string (nullable = true)\n",
      " |-- Longitude: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read in the temperature data \n",
    "temperature_data = '../../data2/GlobalLandTemperaturesByCity.csv'\n",
    "\n",
    "spark_temperature_data = spark.read.format(\"csv\").option(\"header\", \"true\").load(temperature_data)\n",
    "spark_temperature_data.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+-----------------------------+-----+-------+--------+---------+\n",
      "|        dt|AverageTemperature|AverageTemperatureUncertainty| City|Country|Latitude|Longitude|\n",
      "+----------+------------------+-----------------------------+-----+-------+--------+---------+\n",
      "|1743-11-01|             6.068|           1.7369999999999999|Århus|Denmark|  57.05N|   10.33E|\n",
      "|1743-12-01|              null|                         null|Århus|Denmark|  57.05N|   10.33E|\n",
      "|1744-01-01|              null|                         null|Århus|Denmark|  57.05N|   10.33E|\n",
      "|1744-02-01|              null|                         null|Århus|Denmark|  57.05N|   10.33E|\n",
      "|1744-03-01|              null|                         null|Århus|Denmark|  57.05N|   10.33E|\n",
      "|1744-04-01|5.7879999999999985|           3.6239999999999997|Århus|Denmark|  57.05N|   10.33E|\n",
      "|1744-05-01|            10.644|           1.2830000000000001|Århus|Denmark|  57.05N|   10.33E|\n",
      "|1744-06-01|14.050999999999998|                        1.347|Århus|Denmark|  57.05N|   10.33E|\n",
      "|1744-07-01|            16.082|                        1.396|Århus|Denmark|  57.05N|   10.33E|\n",
      "|1744-08-01|              null|                         null|Århus|Denmark|  57.05N|   10.33E|\n",
      "+----------+------------------+-----------------------------+-----+-------+--------+---------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_temperature_data.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temperature Rows count : 8599212\n",
      "Temperature Columns count : 7\n"
     ]
    }
   ],
   "source": [
    "# Get row count\n",
    "rows_temperature = spark_temperature_data.count()\n",
    "print(f\"Temperature Rows count : {rows_temperature}\")\n",
    "\n",
    "# Get columns count\n",
    "cols_temperature = len(spark_temperature_data.columns)\n",
    "print(f\"Temperature Columns count : {cols_temperature}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### Immigration data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- cicid: string (nullable = true)\n",
      " |-- i94yr: string (nullable = true)\n",
      " |-- i94mon: string (nullable = true)\n",
      " |-- i94cit: string (nullable = true)\n",
      " |-- i94res: string (nullable = true)\n",
      " |-- i94port: string (nullable = true)\n",
      " |-- arrdate: string (nullable = true)\n",
      " |-- i94mode: string (nullable = true)\n",
      " |-- i94addr: string (nullable = true)\n",
      " |-- depdate: string (nullable = true)\n",
      " |-- i94bir: string (nullable = true)\n",
      " |-- i94visa: string (nullable = true)\n",
      " |-- count: string (nullable = true)\n",
      " |-- dtadfile: string (nullable = true)\n",
      " |-- visapost: string (nullable = true)\n",
      " |-- occup: string (nullable = true)\n",
      " |-- entdepa: string (nullable = true)\n",
      " |-- entdepd: string (nullable = true)\n",
      " |-- entdepu: string (nullable = true)\n",
      " |-- matflag: string (nullable = true)\n",
      " |-- biryear: string (nullable = true)\n",
      " |-- dtaddto: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- insnum: string (nullable = true)\n",
      " |-- airline: string (nullable = true)\n",
      " |-- admnum: string (nullable = true)\n",
      " |-- fltno: string (nullable = true)\n",
      " |-- visatype: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read in the immigration data \n",
    "immigration_data = 'immigration_data_sample.csv'\n",
    "\n",
    "spark_immigration_data = spark.read.format(\"csv\").option(\"header\", \"true\").load(immigration_data)\n",
    "spark_immigration_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+-------------+-----+--------+\n",
      "|    _c0|    cicid| i94yr|i94mon|i94cit|i94res|i94port|arrdate|i94mode|i94addr|depdate|i94bir|i94visa|count|dtadfile|visapost|occup|entdepa|entdepd|entdepu|matflag|biryear| dtaddto|gender|insnum|airline|       admnum|fltno|visatype|\n",
      "+-------+---------+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+-------------+-----+--------+\n",
      "|2027561|4084316.0|2016.0|   4.0| 209.0| 209.0|    HHW|20566.0|    1.0|     HI|20573.0|  61.0|    2.0|  1.0|20160422|    null| null|      G|      O|   null|      M| 1955.0|07202016|     F|  null|     JL|56582674633.0|00782|      WT|\n",
      "|2171295|4422636.0|2016.0|   4.0| 582.0| 582.0|    MCA|20567.0|    1.0|     TX|20568.0|  26.0|    2.0|  1.0|20160423|     MTR| null|      G|      R|   null|      M| 1990.0|10222016|     M|  null|    *GA|94361995930.0|XBLNG|      B2|\n",
      "| 589494|1195600.0|2016.0|   4.0| 148.0| 112.0|    OGG|20551.0|    1.0|     FL|20571.0|  76.0|    2.0|  1.0|20160407|    null| null|      G|      O|   null|      M| 1940.0|07052016|     M|  null|     LH|55780468433.0|00464|      WT|\n",
      "|2631158|5291768.0|2016.0|   4.0| 297.0| 297.0|    LOS|20572.0|    1.0|     CA|20581.0|  25.0|    2.0|  1.0|20160428|     DOH| null|      G|      O|   null|      M| 1991.0|10272016|     M|  null|     QR|94789696030.0|00739|      B2|\n",
      "|3032257| 985523.0|2016.0|   4.0| 111.0| 111.0|    CHM|20550.0|    3.0|     NY|20553.0|  19.0|    2.0|  1.0|20160406|    null| null|      Z|      K|   null|      M| 1997.0|07042016|     F|  null|   null|42322572633.0| LAND|      WT|\n",
      "| 721257|1481650.0|2016.0|   4.0| 577.0| 577.0|    ATL|20552.0|    1.0|     GA|20606.0|  51.0|    2.0|  1.0|20160408|    null| null|      T|      N|   null|      M| 1965.0|10072016|     M|  null|     DL|  736852585.0|  910|      B2|\n",
      "|1072780|2197173.0|2016.0|   4.0| 245.0| 245.0|    SFR|20556.0|    1.0|     CA|20635.0|  48.0|    2.0|  1.0|20160412|    null| null|      T|      O|   null|      M| 1968.0|10112016|     F|  null|     CX|  786312185.0|  870|      B2|\n",
      "| 112205| 232708.0|2016.0|   4.0| 113.0| 135.0|    NYC|20546.0|    1.0|     NY|20554.0|  33.0|    2.0|  1.0|20160402|    null| null|      G|      O|   null|      M| 1983.0|06302016|     F|  null|     BA|55474485033.0|00117|      WT|\n",
      "|2577162|5227851.0|2016.0|   4.0| 131.0| 131.0|    CHI|20572.0|    1.0|     IL|20575.0|  39.0|    2.0|  1.0|20160428|    null| null|      O|      O|   null|      M| 1977.0|07262016|  null|  null|     LX|59413424733.0|00008|      WT|\n",
      "|  10930|  13213.0|2016.0|   4.0| 116.0| 116.0|    LOS|20545.0|    1.0|     CA|20553.0|  35.0|    2.0|  1.0|20160401|    null| null|      O|      O|   null|      M| 1981.0|06292016|  null|  null|     AA|55449792933.0|00109|      WT|\n",
      "+-------+---------+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+-------------+-----+--------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_immigration_data.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Immigration Rows count : 1000\n",
      "Immigration Columns count : 29\n"
     ]
    }
   ],
   "source": [
    "# Get row count\n",
    "rows_immigration = spark_immigration_data.count()\n",
    "print(f\"Immigration Rows count : {rows_immigration}\")\n",
    "\n",
    "# Get columns count\n",
    "cols_immigration = len(spark_immigration_data.columns)\n",
    "print(f\"Immigration Columns count : {cols_immigration}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### Demographic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- City: string (nullable = true)\n",
      " |-- State: string (nullable = true)\n",
      " |-- Median Age: string (nullable = true)\n",
      " |-- Male Population: string (nullable = true)\n",
      " |-- Female Population: string (nullable = true)\n",
      " |-- Total Population: string (nullable = true)\n",
      " |-- Number of Veterans: string (nullable = true)\n",
      " |-- Foreign-born: string (nullable = true)\n",
      " |-- Average Household Size: string (nullable = true)\n",
      " |-- State Code: string (nullable = true)\n",
      " |-- Race: string (nullable = true)\n",
      " |-- Count: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read in the demographic data \n",
    "demographic_data = 'us-cities-demographics.csv'\n",
    "\n",
    "spark_demographic_data = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"delimiter\", \";\").load(demographic_data)\n",
    "spark_demographic_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+--------------+----------+---------------+-----------------+----------------+------------------+------------+----------------------+----------+--------------------+-----+\n",
      "|            City|         State|Median Age|Male Population|Female Population|Total Population|Number of Veterans|Foreign-born|Average Household Size|State Code|                Race|Count|\n",
      "+----------------+--------------+----------+---------------+-----------------+----------------+------------------+------------+----------------------+----------+--------------------+-----+\n",
      "|   Silver Spring|      Maryland|      33.8|          40601|            41862|           82463|              1562|       30908|                   2.6|        MD|  Hispanic or Latino|25924|\n",
      "|          Quincy| Massachusetts|      41.0|          44129|            49500|           93629|              4147|       32935|                  2.39|        MA|               White|58723|\n",
      "|          Hoover|       Alabama|      38.5|          38040|            46799|           84839|              4819|        8229|                  2.58|        AL|               Asian| 4759|\n",
      "|Rancho Cucamonga|    California|      34.5|          88127|            87105|          175232|              5821|       33878|                  3.18|        CA|Black or African-...|24437|\n",
      "|          Newark|    New Jersey|      34.6|         138040|           143873|          281913|              5829|       86253|                  2.73|        NJ|               White|76402|\n",
      "|          Peoria|      Illinois|      33.1|          56229|            62432|          118661|              6634|        7517|                   2.4|        IL|American Indian a...| 1343|\n",
      "|        Avondale|       Arizona|      29.1|          38712|            41971|           80683|              4815|        8355|                  3.18|        AZ|Black or African-...|11592|\n",
      "|     West Covina|    California|      39.8|          51629|            56860|          108489|              3800|       37038|                  3.56|        CA|               Asian|32716|\n",
      "|        O'Fallon|      Missouri|      36.0|          41762|            43270|           85032|              5783|        3269|                  2.77|        MO|  Hispanic or Latino| 2583|\n",
      "|      High Point|North Carolina|      35.5|          51751|            58077|          109828|              5204|       16315|                  2.65|        NC|               Asian|11060|\n",
      "+----------------+--------------+----------+---------------+-----------------+----------------+------------------+------------+----------------------+----------+--------------------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_demographic_data.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Demographic Rows count : 2891\n",
      "Demographic Columns count : 12\n"
     ]
    }
   ],
   "source": [
    "# Get row count\n",
    "rows_demo = spark_demographic_data.count()\n",
    "print(f\"Demographic Rows count : {rows_demo}\")\n",
    "\n",
    "# Get columns count\n",
    "cols_demo = len(spark_demographic_data.columns)\n",
    "print(f\"Demographic Columns count : {cols_demo}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### Airport data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- continent: string (nullable = true)\n",
      " |-- coordinates: string (nullable = true)\n",
      " |-- elevation_ft: string (nullable = true)\n",
      " |-- gps_code: string (nullable = true)\n",
      " |-- iata_code: string (nullable = true)\n",
      " |-- ident: string (nullable = true)\n",
      " |-- iso_country: string (nullable = true)\n",
      " |-- iso_region: string (nullable = true)\n",
      " |-- local_code: string (nullable = true)\n",
      " |-- municipality: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read in the airport data \n",
    "#airport_data = 'airport-codes_csv.csv'\n",
    "airport_data = 'airport-codes_json.json'\n",
    "\n",
    "#spark_airport_data = spark.read.format(\"csv\").option(\"header\", \"true\").load(airport_data)\n",
    "spark_airport_data = spark.read.format(\"json\").option(\"header\", \"true\").load(airport_data)\n",
    "\n",
    "spark_airport_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+------------+--------+---------+-----+-----------+----------+----------+------------+--------------------+-------------+\n",
      "|continent|         coordinates|elevation_ft|gps_code|iata_code|ident|iso_country|iso_region|local_code|municipality|                name|         type|\n",
      "+---------+--------------------+------------+--------+---------+-----+-----------+----------+----------+------------+--------------------+-------------+\n",
      "|       NA|-74.9336013793945...|          11|     00A|     null|  00A|         US|     US-PA|       00A|    Bensalem|   Total Rf Heliport|     heliport|\n",
      "|       NA|-101.473911, 38.7...|        3435|    00AA|     null| 00AA|         US|     US-KS|      00AA|       Leoti|Aero B Ranch Airport|small_airport|\n",
      "|       NA|-151.695999146, 5...|         450|    00AK|     null| 00AK|         US|     US-AK|      00AK|Anchor Point|        Lowell Field|small_airport|\n",
      "|       NA|-86.7703018188476...|         820|    00AL|     null| 00AL|         US|     US-AL|      00AL|     Harvest|        Epps Airpark|small_airport|\n",
      "|       NA| -91.254898, 35.6087|         237|    null|     null| 00AR|         US|     US-AR|      null|     Newport|Newport Hospital ...|       closed|\n",
      "|       NA|-97.8180194, 34.9...|        1100|    00AS|     null| 00AS|         US|     US-OK|      00AS|        Alex|      Fulton Airport|small_airport|\n",
      "|       NA|-112.165000915527...|        3810|    00AZ|     null| 00AZ|         US|     US-AZ|      00AZ|      Cordes|      Cordes Airport|small_airport|\n",
      "|       NA|-116.888000488, 3...|        3038|    00CA|     null| 00CA|         US|     US-CA|      00CA|     Barstow|Goldstone /Gts/ A...|small_airport|\n",
      "|       NA|-121.763427, 39.4...|          87|    00CL|     null| 00CL|         US|     US-CA|      00CL|       Biggs| Williams Ag Airport|small_airport|\n",
      "|       NA|-116.4597417, 32....|        3350|    00CN|     null| 00CN|         US|     US-CA|      00CN| Pine Valley|Kitchen Creek Hel...|     heliport|\n",
      "+---------+--------------------+------------+--------+---------+-----+-----------+----------+----------+------------+--------------------+-------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_airport_data.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Airport Rows count : 57421\n",
      "Airport Columns count : 12\n"
     ]
    }
   ],
   "source": [
    "# Get row count\n",
    "rows_airport = spark_airport_data.count()\n",
    "print(f\"Airport Rows count : {rows_airport}\")\n",
    "\n",
    "# Get columns count\n",
    "cols_airport = len(spark_airport_data.columns)\n",
    "print(f\"Airport Columns count : {cols_airport}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### Airport codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- airport_code: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- state_code: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read in the airport codes (from label file)\n",
    "airport_codes = 'airport_codes.csv'\n",
    "\n",
    "spark_airport_codes = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"delimiter\", \";\").load(airport_codes)\n",
    "spark_airport_codes.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------------+---------------+\n",
      "|airport_code|                city|     state_code|\n",
      "+------------+--------------------+---------------+\n",
      "|         ALC|               ALCAN|AK             |\n",
      "|         ANC|           ANCHORAGE|    AK         |\n",
      "|         BAR|BAKER AAF - BAKER...|             AK|\n",
      "|         DAC|       DALTONS CACHE|        AK     |\n",
      "|         PIZ|DEW STATION PT LA...|             AK|\n",
      "+------------+--------------------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_airport_codes.show(5)\n",
    "spark_airport_codes.createOrReplaceTempView('airport_codes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### Country codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Code: string (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read in the country codes (from label file)\n",
    "country_codes = 'country_codes.csv'\n",
    "\n",
    "spark_country_codes = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"delimiter\", \";\").load(country_codes)\n",
    "spark_country_codes.printSchema()\n",
    "spark_country_codes=spark_country_codes.withColumn(\"Code\", spark_country_codes.Code+\".0\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "| Code|             Country|\n",
      "+-----+--------------------+\n",
      "|582.0|  MEXICO Air Sea,...|\n",
      "|236.0|         AFGHANISTAN|\n",
      "|101.0|             ALBANIA|\n",
      "|316.0|             ALGERIA|\n",
      "|102.0|             ANDORRA|\n",
      "+-----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_country_codes.show(5)\n",
    "spark_country_codes.createOrReplaceTempView('country_codes')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#write to parquet\n",
    "#df_spark.write.parquet(\"sas_data\")\n",
    "#df_spark=spark.read.parquet(\"sas_data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 2: Explore and Assess the Data\n",
    "#### Explore the Data \n",
    "Identify data quality issues, like missing values, duplicate data, etc.\n",
    "\n",
    "#### Cleaning Steps\n",
    "Document steps necessary to clean the data\n",
    "* Check how many null/nan values are in the tables\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-----+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+-------+------+------+-------+------+-----+--------+\n",
      "|_c0|cicid|i94yr|i94mon|i94cit|i94res|i94port|arrdate|i94mode|i94addr|depdate|i94bir|i94visa|count|dtadfile|visapost|occup|entdepa|entdepd|entdepu|matflag|biryear|dtaddto|gender|insnum|airline|admnum|fltno|visatype|\n",
      "+---+-----+-----+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+-------+------+------+-------+------+-----+--------+\n",
      "|  0|    0|    0|     0|     0|     0|      0|      0|      0|     59|     49|     0|      0|    0|       0|     618|  996|      0|     46|   1000|     46|      0|      0|   141|   965|     33|     0|    8|       0|\n",
      "+---+-----+-----+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+-------+------+------+-------+------+-----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_immigration_data.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in spark_immigration_data.columns]).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------+-----------------------------+----+-------+--------+---------+\n",
      "| dt|AverageTemperature|AverageTemperatureUncertainty|City|Country|Latitude|Longitude|\n",
      "+---+------------------+-----------------------------+----+-------+--------+---------+\n",
      "|  0|            364130|                       364130|   0|      0|       0|        0|\n",
      "+---+------------------+-----------------------------+----+-------+--------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_temperature_data.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in spark_temperature_data.columns]).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+------------+--------+---------+-----+-----------+----------+----------+------------+----+----+\n",
      "|continent|coordinates|elevation_ft|gps_code|iata_code|ident|iso_country|iso_region|local_code|municipality|name|type|\n",
      "+---------+-----------+------------+--------+---------+-----+-----------+----------+----------+------------+----+----+\n",
      "|        0|          0|        7813|   15860|    48196|    0|          0|         0|     27391|        5894|   0|   0|\n",
      "+---------+-----------+------------+--------+---------+-----+-----------+----------+----------+------------+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_airport_data.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in spark_airport_data.columns]).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+----------+---------------+-----------------+----------------+------------------+------------+----------------------+----------+----+-----+\n",
      "|City|State|Median Age|Male Population|Female Population|Total Population|Number of Veterans|Foreign-born|Average Household Size|State Code|Race|Count|\n",
      "+----+-----+----------+---------------+-----------------+----------------+------------------+------------+----------------------+----------+----+-----+\n",
      "|   0|    0|         0|              3|                3|               0|                13|          13|                    16|         0|   0|    0|\n",
      "+----+-----+----------+---------------+-----------------+----------------+------------------+------------+----------------------+----------+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_demographic_data.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in spark_demographic_data.columns]).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### Immigration data cleaning:\n",
    "* Remove column 'entdepu', because it consists of just null values\n",
    "* Remove column 'occup' and 'insnum', because they nearly consist of just null values\n",
    "* Remove column i94cit, because it is a duplicate of the column i94res \n",
    "* Add n/a to columns where gender is null\n",
    "* Replace the columns arrdate and depdate with actual dates \n",
    "* Create column residence with mapping to the actual country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- cicid: string (nullable = true)\n",
      " |-- i94yr: string (nullable = true)\n",
      " |-- i94mon: string (nullable = true)\n",
      " |-- i94res: string (nullable = true)\n",
      " |-- i94port: string (nullable = true)\n",
      " |-- arrdate: string (nullable = true)\n",
      " |-- i94mode: string (nullable = true)\n",
      " |-- i94addr: string (nullable = true)\n",
      " |-- depdate: string (nullable = true)\n",
      " |-- i94bir: string (nullable = true)\n",
      " |-- i94visa: string (nullable = true)\n",
      " |-- count: string (nullable = true)\n",
      " |-- dtadfile: string (nullable = true)\n",
      " |-- visapost: string (nullable = true)\n",
      " |-- entdepa: string (nullable = true)\n",
      " |-- entdepd: string (nullable = true)\n",
      " |-- matflag: string (nullable = true)\n",
      " |-- biryear: string (nullable = true)\n",
      " |-- dtaddto: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- airline: string (nullable = true)\n",
      " |-- admnum: string (nullable = true)\n",
      " |-- fltno: string (nullable = true)\n",
      " |-- visatype: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#remove entdepu, occup, i94cit from immigration data (consists of just null or is duplicate)\n",
    "clean_imgr_data = spark_immigration_data.drop(\"entdepu\", \"occup\", \"i94cit\", \"insnum\")\n",
    "clean_imgr_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-------+-------+-------+-------+--------+------+-------+-------------+-----+--------+\n",
      "|    _c0|    cicid| i94yr|i94mon|i94res|i94port|arrdate|i94mode|i94addr|depdate|i94bir|i94visa|count|dtadfile|visapost|entdepa|entdepd|matflag|biryear| dtaddto|gender|airline|       admnum|fltno|visatype|\n",
      "+-------+---------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-------+-------+-------+-------+--------+------+-------+-------------+-----+--------+\n",
      "|2027561|4084316.0|2016.0|   4.0| 209.0|    HHW|20566.0|    1.0|     HI|20573.0|  61.0|    2.0|  1.0|20160422|    null|      G|      O|      M| 1955.0|07202016|     F|     JL|56582674633.0|00782|      WT|\n",
      "|2171295|4422636.0|2016.0|   4.0| 582.0|    MCA|20567.0|    1.0|     TX|20568.0|  26.0|    2.0|  1.0|20160423|     MTR|      G|      R|      M| 1990.0|10222016|     M|    *GA|94361995930.0|XBLNG|      B2|\n",
      "| 589494|1195600.0|2016.0|   4.0| 112.0|    OGG|20551.0|    1.0|     FL|20571.0|  76.0|    2.0|  1.0|20160407|    null|      G|      O|      M| 1940.0|07052016|     M|     LH|55780468433.0|00464|      WT|\n",
      "|2631158|5291768.0|2016.0|   4.0| 297.0|    LOS|20572.0|    1.0|     CA|20581.0|  25.0|    2.0|  1.0|20160428|     DOH|      G|      O|      M| 1991.0|10272016|     M|     QR|94789696030.0|00739|      B2|\n",
      "|3032257| 985523.0|2016.0|   4.0| 111.0|    CHM|20550.0|    3.0|     NY|20553.0|  19.0|    2.0|  1.0|20160406|    null|      Z|      K|      M| 1997.0|07042016|     F|   null|42322572633.0| LAND|      WT|\n",
      "+-------+---------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-------+-------+-------+-------+--------+------+-------+-------------+-----+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clean_imgr_data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "clean_imgr_data.createOrReplaceTempView(\"imgr_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-------+-------+-------+-------+--------+------+-------+-------------+-----+--------+------------+--------------+\n",
      "|    _c0|    cicid| i94yr|i94mon|i94res|i94port|arrdate|i94mode|i94addr|depdate|i94bir|i94visa|count|dtadfile|visapost|entdepa|entdepd|matflag|biryear| dtaddto|gender|airline|       admnum|fltno|visatype|arrival_date|departure_date|\n",
      "+-------+---------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-------+-------+-------+-------+--------+------+-------+-------------+-----+--------+------------+--------------+\n",
      "|2027561|4084316.0|2016.0|   4.0| 209.0|    HHW|20566.0|    1.0|     HI|20573.0|  61.0|    2.0|  1.0|20160422|    null|      G|      O|      M| 1955.0|07202016|     F|     JL|56582674633.0|00782|      WT|  2016-04-22|    2016-04-29|\n",
      "|2171295|4422636.0|2016.0|   4.0| 582.0|    MCA|20567.0|    1.0|     TX|20568.0|  26.0|    2.0|  1.0|20160423|     MTR|      G|      R|      M| 1990.0|10222016|     M|    *GA|94361995930.0|XBLNG|      B2|  2016-04-23|    2016-04-24|\n",
      "| 589494|1195600.0|2016.0|   4.0| 112.0|    OGG|20551.0|    1.0|     FL|20571.0|  76.0|    2.0|  1.0|20160407|    null|      G|      O|      M| 1940.0|07052016|     M|     LH|55780468433.0|00464|      WT|  2016-04-07|    2016-04-27|\n",
      "|2631158|5291768.0|2016.0|   4.0| 297.0|    LOS|20572.0|    1.0|     CA|20581.0|  25.0|    2.0|  1.0|20160428|     DOH|      G|      O|      M| 1991.0|10272016|     M|     QR|94789696030.0|00739|      B2|  2016-04-28|    2016-05-07|\n",
      "|3032257| 985523.0|2016.0|   4.0| 111.0|    CHM|20550.0|    3.0|     NY|20553.0|  19.0|    2.0|  1.0|20160406|    null|      Z|      K|      M| 1997.0|07042016|     F|   null|42322572633.0| LAND|      WT|  2016-04-06|    2016-04-09|\n",
      "+-------+---------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-------+-------+-------+-------+--------+------+-------+-------------+-----+--------+------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#create new columns arrival_date and departure_date that consist of the actual dates \n",
    "clean_imgr_data_2 = spark.sql(\"SELECT *, date_add(to_date('1960-01-01'), arrdate) AS arrival_date FROM imgr_data\")\n",
    "clean_imgr_data_2.createOrReplaceTempView(\"imgr_data\")\n",
    "clean_imgr_data_3 = spark.sql(\"SELECT *, date_add(to_date('1960-01-01'), depdate) AS departure_date FROM imgr_data\")\n",
    "\n",
    "clean_imgr_data_3.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Remove null values from gender\n",
    "clean_imgr_data_3= clean_imgr_data_3.na.fill(value='n/a',subset=[\"gender\", \"i94addr\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "clean_imgr_data_3.createOrReplaceTempView(\"imgr_data\")\n",
    "\n",
    "clean_imgr_data_4 = spark.sql(\"SELECT *  FROM imgr_data WHERE arrival_date IS NOT NULL AND departure_date IS NOT NULL\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#drop the columns arrdate and depdate\n",
    "clean_imgr_data = clean_imgr_data_4.drop(col(\"arrdate\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- cicid: string (nullable = true)\n",
      " |-- i94yr: string (nullable = true)\n",
      " |-- i94mon: string (nullable = true)\n",
      " |-- i94res: string (nullable = true)\n",
      " |-- i94port: string (nullable = true)\n",
      " |-- i94mode: string (nullable = true)\n",
      " |-- i94addr: string (nullable = false)\n",
      " |-- i94bir: string (nullable = true)\n",
      " |-- i94visa: string (nullable = true)\n",
      " |-- count: string (nullable = true)\n",
      " |-- dtadfile: string (nullable = true)\n",
      " |-- visapost: string (nullable = true)\n",
      " |-- entdepa: string (nullable = true)\n",
      " |-- entdepd: string (nullable = true)\n",
      " |-- matflag: string (nullable = true)\n",
      " |-- biryear: string (nullable = true)\n",
      " |-- dtaddto: string (nullable = true)\n",
      " |-- gender: string (nullable = false)\n",
      " |-- airline: string (nullable = true)\n",
      " |-- admnum: string (nullable = true)\n",
      " |-- fltno: string (nullable = true)\n",
      " |-- visatype: string (nullable = true)\n",
      " |-- arrival_date: date (nullable = true)\n",
      " |-- departure_date: date (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cl_imgr_data = clean_imgr_data.drop(col(\"depdate\"))\n",
    "cl_imgr_data.printSchema()\n",
    "cl_imgr_data.createOrReplaceTempView(\"imgr_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "cl_imgr_data_final = spark.sql(\"\"\"\n",
    "SELECT i.*, cc.Country as residence\n",
    "FROM imgr_data as i\n",
    "JOIN country_codes as cc\n",
    "ON i.i94res  = cc.Code  \n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+------+------+------+-------+-------+-------+------+-------+-----+--------+--------+-------+-------+-------+-------+--------+------+-------+-------------+-----+--------+------------+--------------+--------------------+\n",
      "|    _c0|    cicid| i94yr|i94mon|i94res|i94port|i94mode|i94addr|i94bir|i94visa|count|dtadfile|visapost|entdepa|entdepd|matflag|biryear| dtaddto|gender|airline|       admnum|fltno|visatype|arrival_date|departure_date|           residence|\n",
      "+-------+---------+------+------+------+-------+-------+-------+------+-------+-----+--------+--------+-------+-------+-------+-------+--------+------+-------+-------------+-----+--------+------------+--------------+--------------------+\n",
      "|2027561|4084316.0|2016.0|   4.0| 209.0|    HHW|    1.0|     HI|  61.0|    2.0|  1.0|20160422|    null|      G|      O|      M| 1955.0|07202016|     F|     JL|56582674633.0|00782|      WT|  2016-04-22|    2016-04-29|               JAPAN|\n",
      "|2171295|4422636.0|2016.0|   4.0| 582.0|    MCA|    1.0|     TX|  26.0|    2.0|  1.0|20160423|     MTR|      G|      R|      M| 1990.0|10222016|     M|    *GA|94361995930.0|XBLNG|      B2|  2016-04-23|    2016-04-24|  MEXICO Air Sea,...|\n",
      "| 589494|1195600.0|2016.0|   4.0| 112.0|    OGG|    1.0|     FL|  76.0|    2.0|  1.0|20160407|    null|      G|      O|      M| 1940.0|07052016|     M|     LH|55780468433.0|00464|      WT|  2016-04-07|    2016-04-27|             GERMANY|\n",
      "|2631158|5291768.0|2016.0|   4.0| 297.0|    LOS|    1.0|     CA|  25.0|    2.0|  1.0|20160428|     DOH|      G|      O|      M| 1991.0|10272016|     M|     QR|94789696030.0|00739|      B2|  2016-04-28|    2016-05-07|               QATAR|\n",
      "|3032257| 985523.0|2016.0|   4.0| 111.0|    CHM|    3.0|     NY|  19.0|    2.0|  1.0|20160406|    null|      Z|      K|      M| 1997.0|07042016|     F|   null|42322572633.0| LAND|      WT|  2016-04-06|    2016-04-09|              FRANCE|\n",
      "| 721257|1481650.0|2016.0|   4.0| 577.0|    ATL|    1.0|     GA|  51.0|    2.0|  1.0|20160408|    null|      T|      N|      M| 1965.0|10072016|     M|     DL|  736852585.0|  910|      B2|  2016-04-08|    2016-06-01|           GUATEMALA|\n",
      "|1072780|2197173.0|2016.0|   4.0| 245.0|    SFR|    1.0|     CA|  48.0|    2.0|  1.0|20160412|    null|      T|      O|      M| 1968.0|10112016|     F|     CX|  786312185.0|  870|      B2|  2016-04-12|    2016-06-30|          CHINA, PRC|\n",
      "| 112205| 232708.0|2016.0|   4.0| 135.0|    NYC|    1.0|     NY|  33.0|    2.0|  1.0|20160402|    null|      G|      O|      M| 1983.0|06302016|     F|     BA|55474485033.0|00117|      WT|  2016-04-02|    2016-04-10|      UNITED KINGDOM|\n",
      "|2577162|5227851.0|2016.0|   4.0| 131.0|    CHI|    1.0|     IL|  39.0|    2.0|  1.0|20160428|    null|      O|      O|      M| 1977.0|07262016|   n/a|     LX|59413424733.0|00008|      WT|  2016-04-28|    2016-05-01|         SWITZERLAND|\n",
      "|  10930|  13213.0|2016.0|   4.0| 116.0|    LOS|    1.0|     CA|  35.0|    2.0|  1.0|20160401|    null|      O|      O|      M| 1981.0|06292016|   n/a|     AA|55449792933.0|00109|      WT|  2016-04-01|    2016-04-09|             IRELAND|\n",
      "+-------+---------+------+------+------+-------+-------+-------+------+-------+-----+--------+--------+-------+-------+-------+-------+--------+------+-------+-------------+-----+--------+------------+--------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cl_imgr_data_final.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "cl_imgr_data_final.createOrReplaceTempView('imgr_data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### Temperature data cleaning:\n",
    "* Remove all entries that are not in the US\n",
    "* Remove entries where AverageTemperature is null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "spark_temperature_data.createOrReplaceTempView('temp_data')\n",
    "\n",
    "clean_temp_data = spark.sql(\"\"\"\n",
    "SELECT * \n",
    "FROM temp_data\n",
    "WHERE Country = 'United States' AND AverageTemperature IS NOT NULL\n",
    "\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+-----------------------------+-------+-------------+--------+---------+\n",
      "|        dt|AverageTemperature|AverageTemperatureUncertainty|   City|      Country|Latitude|Longitude|\n",
      "+----------+------------------+-----------------------------+-------+-------------+--------+---------+\n",
      "|1820-01-01|2.1010000000000004|                        3.217|Abilene|United States|  32.95N|  100.53W|\n",
      "|1820-02-01|             6.926|                        2.853|Abilene|United States|  32.95N|  100.53W|\n",
      "|1820-03-01|            10.767|                        2.395|Abilene|United States|  32.95N|  100.53W|\n",
      "|1820-04-01|17.988999999999994|                        2.202|Abilene|United States|  32.95N|  100.53W|\n",
      "|1820-05-01|            21.809|                        2.036|Abilene|United States|  32.95N|  100.53W|\n",
      "+----------+------------------+-----------------------------+-------+-------------+--------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clean_temp_data.show(5)\n",
    "clean_temp_data.createOrReplaceTempView('temp_data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### Airport data cleaning:\n",
    "* Remove all entries that are not in the US\n",
    "* Split coordinates into longitude and latitude\n",
    "* Split iso_region into country and state \n",
    "* Replace null values of elevation_ft with n/a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Remove airports that are not in the US\n",
    "spark_airport_data.createOrReplaceTempView('airport_data')\n",
    "\n",
    "clean_airport_data = spark.sql(\"\"\"\n",
    "SELECT * \n",
    "FROM airport_data\n",
    "WHERE iso_country = 'US' AND continent = 'NA' AND elevation_ft IS NOT NULL\n",
    "\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+------------+--------+---------+-----+-----------+----------+----------+------------+--------------------+-------------+------------------+---------------+\n",
      "|continent|         coordinates|elevation_ft|gps_code|iata_code|ident|iso_country|iso_region|local_code|municipality|                name|         type|         longitude|       latitude|\n",
      "+---------+--------------------+------------+--------+---------+-----+-----------+----------+----------+------------+--------------------+-------------+------------------+---------------+\n",
      "|       NA|-74.9336013793945...|          11|     00A|     null|  00A|         US|     US-PA|       00A|    Bensalem|   Total Rf Heliport|     heliport|-74.93360137939453| 40.07080078125|\n",
      "|       NA|-101.473911, 38.7...|        3435|    00AA|     null| 00AA|         US|     US-KS|      00AA|       Leoti|Aero B Ranch Airport|small_airport|       -101.473911|      38.704022|\n",
      "+---------+--------------------+------------+--------+---------+-----+-----------+----------+----------+------------+--------------------+-------------+------------------+---------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Create columns for longitude and latitude of the coordinates\n",
    "split_col = split(clean_airport_data['coordinates'], ',')\n",
    "clean_airport_data = clean_airport_data.withColumn('longitude', split_col[0])\n",
    "clean_airport_data = clean_airport_data.withColumn('latitude', split_col[1])\n",
    "\n",
    "clean_airport_data.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+------------+--------+---------+-----+-----------+----------+----------+------------+--------------------+-------------+------------------+---------------+----------+\n",
      "|continent|         coordinates|elevation_ft|gps_code|iata_code|ident|iso_country|iso_region|local_code|municipality|                name|         type|         longitude|       latitude|state_code|\n",
      "+---------+--------------------+------------+--------+---------+-----+-----------+----------+----------+------------+--------------------+-------------+------------------+---------------+----------+\n",
      "|       NA|-74.9336013793945...|          11|     00A|     null|  00A|         US|     US-PA|       00A|    Bensalem|   Total Rf Heliport|     heliport|-74.93360137939453| 40.07080078125|        PA|\n",
      "|       NA|-101.473911, 38.7...|        3435|    00AA|     null| 00AA|         US|     US-KS|      00AA|       Leoti|Aero B Ranch Airport|small_airport|       -101.473911|      38.704022|        KS|\n",
      "+---------+--------------------+------------+--------+---------+-----+-----------+----------+----------+------------+--------------------+-------------+------------------+---------------+----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Create column with state code\n",
    "split_col = split(clean_airport_data['iso_region'], '-')\n",
    "clean_airport_data = clean_airport_data.withColumn('state_code', split_col[1])\n",
    "\n",
    "clean_airport_data.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[continent: string, coordinates: string, elevation_ft: string, gps_code: string, iata_code: string, ident: string, iso_country: string, iso_region: string, local_code: string, municipality: string, name: string, type: string, longitude: string, latitude: string, state_code: string]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Replace null values in elevation_ft\n",
    "clean_airport_data.na.fill(value='n/a',subset=[\"elevation_ft\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "clean_airport_data.createOrReplaceTempView('airport_data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### Demographic data cleaning:\n",
    "* Compressing because race is not needed\n",
    "* Replace null values in Foreign-born, Male Population and Female Population with n/a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "spark_demographic_data.createOrReplaceTempView('demog_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Removing the columns Race and Count\n",
    "clean_demog_data = spark_demographic_data.drop(\"Race\", \"Count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Replace null values in foreign-born, male/female population\n",
    "clean_demog_data=clean_demog_data.na.fill(value='n/a',subset=[\"Foreign-born\", \"Male Population\", \"Female Population\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-------------+----------+---------------+-----------------+----------------+------------------+------------+----------------------+----------+\n",
      "|            City|        State|Median Age|Male Population|Female Population|Total Population|Number of Veterans|Foreign-born|Average Household Size|State Code|\n",
      "+----------------+-------------+----------+---------------+-----------------+----------------+------------------+------------+----------------------+----------+\n",
      "|   Silver Spring|     Maryland|      33.8|          40601|            41862|           82463|              1562|       30908|                   2.6|        MD|\n",
      "|          Quincy|Massachusetts|      41.0|          44129|            49500|           93629|              4147|       32935|                  2.39|        MA|\n",
      "|          Hoover|      Alabama|      38.5|          38040|            46799|           84839|              4819|        8229|                  2.58|        AL|\n",
      "|Rancho Cucamonga|   California|      34.5|          88127|            87105|          175232|              5821|       33878|                  3.18|        CA|\n",
      "|          Newark|   New Jersey|      34.6|         138040|           143873|          281913|              5829|       86253|                  2.73|        NJ|\n",
      "+----------------+-------------+----------+---------------+-----------------+----------------+------------------+------------+----------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clean_demog_data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2891"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_demog_data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Dropping duplicates\n",
    "clean_demog_data = clean_demog_data.dropDuplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "596"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_demog_data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "clean_demog_data.createOrReplaceTempView('demog_data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 3: Define the Data Model\n",
    "#### 3.1 Conceptual Data Model\n",
    "Map out the conceptual data model and explain why you chose that model\n",
    "* The data model will be a star schema. Therefore dimension tables are created that contain information. A fact table is created which stores facts that connect to the dimension tables. The star schema makes it easy to execute queries and make all the information accessible for multiple types of users.\n",
    "* The fact table includes the field duration_of_stay, which returns the duration of the stay of the immigrant in the US in days.\n",
    "\n",
    "Conceptual Data Model: \n",
    "\n",
    "![Conceptual Data Model](data_model.png)\n",
    "\n",
    "\n",
    "#### 3.2 Mapping Out Data Pipelines\n",
    "List the steps necessary to pipeline the data into the chosen data model\n",
    "* Loaded data into four staging tables\n",
    "* Create dimension table airport \n",
    "* Create dimension table temperature \n",
    "* Create dimension table demographics \n",
    "* Create dimension table immigrant\n",
    "* Create dimension table state\n",
    "* Create dimension table visa\n",
    "* Create fact table immigration_cases_fact\n",
    "* Write all tables into parquet files\n",
    "* Run data quality checks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model\n",
    "Build the data pipelines to create the data model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### Create dimension table visa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Create dimension table visa\n",
    "dim_visa = spark.sql(\"\"\"\n",
    "SELECT DISTINCT i94visa as visa_type, \n",
    "    CASE\n",
    "    WHEN i94visa= 1.0 THEN 'Business'\n",
    "    WHEN i94visa= 2.0 THEN 'Pleasure'\n",
    "    WHEN i94visa= 3.0 THEN 'Student' END as visa_description\n",
    "FROM imgr_data \n",
    "\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------------+\n",
      "|visa_type|visa_description|\n",
      "+---------+----------------+\n",
      "|      3.0|         Student|\n",
      "|      1.0|        Business|\n",
      "|      2.0|        Pleasure|\n",
      "+---------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dim_visa.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Write to parquet files\n",
    "dim_visa.write.mode(\"overwrite\").parquet(\"dim_visa\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### Create dimension table immigrant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Create dimension table immigrant\n",
    "dim_immigrant = spark.sql(\"\"\"\n",
    "SELECT DISTINCT cicid as cic_id, \n",
    "   gender, \n",
    "   biryear as birthyear, \n",
    "   i94bir as age\n",
    "FROM imgr_data\n",
    "\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+---------+----+\n",
      "|   cic_id|gender|birthyear| age|\n",
      "+---------+------+---------+----+\n",
      "|3426228.0|     F|   1998.0|18.0|\n",
      "|2758683.0|     F|   1985.0|31.0|\n",
      "|5471104.0|     M|   1939.0|77.0|\n",
      "|3085571.0|     F|   1984.0|32.0|\n",
      "| 211301.0|     F|   1973.0|43.0|\n",
      "+---------+------+---------+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dim_immigrant.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Write to parquet files\n",
    "dim_immigrant.write.mode(\"overwrite\").parquet(\"dim_immigrant\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### Create dimension table state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Create dimension table state\n",
    "dim_state = spark.sql(\"\"\"\n",
    "SELECT DISTINCT d.`State Code` as state_code, \n",
    "    d.State as state_name, \n",
    "    a.iso_country as country,\n",
    "    a.continent as continent\n",
    "FROM airport_data as a, demog_data as d\n",
    "WHERE a.state_code = d.`State Code` \n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+-------+---------+\n",
      "|state_code|  state_name|country|continent|\n",
      "+----------+------------+-------+---------+\n",
      "|        AK|      Alaska|     US|       NA|\n",
      "|        MI|    Michigan|     US|       NA|\n",
      "|        OH|        Ohio|     US|       NA|\n",
      "|        TN|   Tennessee|     US|       NA|\n",
      "|        UT|        Utah|     US|       NA|\n",
      "|        CT| Connecticut|     US|       NA|\n",
      "|        NM|  New Mexico|     US|       NA|\n",
      "|        AR|    Arkansas|     US|       NA|\n",
      "|        ND|North Dakota|     US|       NA|\n",
      "|        NE|    Nebraska|     US|       NA|\n",
      "|        NY|    New York|     US|       NA|\n",
      "|        MS| Mississippi|     US|       NA|\n",
      "|        ID|       Idaho|     US|       NA|\n",
      "|        IN|     Indiana|     US|       NA|\n",
      "|        IL|    Illinois|     US|       NA|\n",
      "|        OK|    Oklahoma|     US|       NA|\n",
      "|        GA|     Georgia|     US|       NA|\n",
      "|        NV|      Nevada|     US|       NA|\n",
      "|        IA|        Iowa|     US|       NA|\n",
      "|        KS|      Kansas|     US|       NA|\n",
      "+----------+------------+-------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dim_state.show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Write to parquet files\n",
    "dim_state.write.mode(\"overwrite\").parquet(\"dim_state\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### Create dimension table demographics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Create dimension table demographics\n",
    "dim_demographics = spark.sql(\"\"\"\n",
    "SELECT City as city, \n",
    "    State as state, \n",
    "    `Total Population` as total_population, \n",
    "    `Male Population` as male_population, \n",
    "    `Female Population` as female_population, \n",
    "    `Foreign-Born` as foreign_born, \n",
    "    `Median Age` as median_age\n",
    "FROM demog_data \n",
    "\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------------+----------------+---------------+-----------------+------------+----------+\n",
      "|           city|         state|total_population|male_population|female_population|foreign_born|median_age|\n",
      "+---------------+--------------+----------------+---------------+-----------------+------------+----------+\n",
      "|   Johnson City|     Tennessee|           65369|          31019|            34350|        2878|      38.2|\n",
      "|   San Clemente|    California|           65532|          34076|            31456|        8109|      45.2|\n",
      "|   Redwood City|    California|           85300|          42676|            42624|       27652|      37.1|\n",
      "|     Fort Myers|       Florida|           74015|          36850|            37165|       15365|      37.3|\n",
      "|West Palm Beach|       Florida|          106782|          49262|            57520|       30675|      39.6|\n",
      "|      San Diego|    California|         1394907|         693826|           701081|      373842|      34.5|\n",
      "|         German|      Maryland|           84122|          41115|            43007|       27877|      34.9|\n",
      "|          Pharr|         Texas|           76535|          35242|            41293|       25532|      26.9|\n",
      "|      Harlingen|         Texas|           65769|          32404|            33365|       10391|      30.1|\n",
      "|     Palm Coast|       Florida|           82121|          39489|            42632|        9084|      48.8|\n",
      "|       Bismarck|  North Dakota|           70240|          34675|            35565|        2064|      38.0|\n",
      "|      Asheville|North Carolina|           88507|          42100|            46407|        6630|      37.9|\n",
      "|    San Antonio|         Texas|         1469824|         721405|           748419|      208046|      33.1|\n",
      "|       Hesperia|    California|           93286|          43588|            49698|       16667|      29.4|\n",
      "|     Alexandria|      Virginia|          153511|          74989|            78522|       44030|      36.6|\n",
      "|     Wilmington|      Delaware|           71957|          32680|            39277|        3336|      36.4|\n",
      "|       Plymouth|     Minnesota|           75928|          39035|            36893|        8830|      38.4|\n",
      "|  Santa Clarita|    California|          182367|          90192|            92175|       40666|      38.1|\n",
      "|       Rockford|      Illinois|          149346|          71076|            78270|       18323|      36.3|\n",
      "|     Union City|    California|           74510|          38599|            35911|       32752|      38.5|\n",
      "+---------------+--------------+----------------+---------------+-----------------+------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dim_demographics.show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Write to parquet files\n",
    "dim_demographics.write.mode(\"overwrite\").parquet(\"dim_demographics\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### Create dimension table airport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Create dimension table airport\n",
    "dim_airport = spark.sql(\"\"\"\n",
    "SELECT a.ident as airport_code, \n",
    "    a.state_code as state_code,\n",
    "    a.type as airport_type,\n",
    "    a.elevation_ft,\n",
    "    a.name as airport_name, \n",
    "    a.longitude, \n",
    "    a.latitude\n",
    "FROM airport_data as a\n",
    "WHERE a.type ='small_airport' OR a.type ='medium_airport' OR a.type ='large_airport'\n",
    "SORT BY airport_code asc\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------+-------------+------------+--------------------+-------------------+-------------------+\n",
      "|airport_code|state_code| airport_type|elevation_ft|        airport_name|          longitude|           latitude|\n",
      "+------------+----------+-------------+------------+--------------------+-------------------+-------------------+\n",
      "|        00AA|        KS|small_airport|        3435|Aero B Ranch Airport|        -101.473911|          38.704022|\n",
      "|        00AK|        AK|small_airport|         450|        Lowell Field|     -151.695999146|        59.94919968|\n",
      "|        00AL|        AL|small_airport|         820|        Epps Airpark| -86.77030181884766|  34.86479949951172|\n",
      "|        00AS|        OK|small_airport|        1100|      Fulton Airport|        -97.8180194|         34.9428028|\n",
      "|        00AZ|        AZ|small_airport|        3810|      Cordes Airport|-112.16500091552734| 34.305599212646484|\n",
      "|        00CA|        CA|small_airport|        3038|Goldstone /Gts/ A...|     -116.888000488| 35.350498199499995|\n",
      "|        00CL|        CA|small_airport|          87| Williams Ag Airport|        -121.763427|          39.427188|\n",
      "|        00FA|        FL|small_airport|          53| Grass Patch Airport| -82.21900177001953|  28.64550018310547|\n",
      "|        00FL|        FL|small_airport|          35|   River Oak Airport| -80.96920013427734| 27.230899810791016|\n",
      "|        00GA|        GA|small_airport|         700|    Lt World Airport| -84.06829833984375|  33.76750183105469|\n",
      "+------------+----------+-------------+------------+--------------------+-------------------+-------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dim_airport.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Write to parquet files\n",
    "dim_airport.write.mode(\"overwrite\").parquet(\"dim_airport\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### Create dimension table temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Create dimension table temperature\n",
    "dim_temperature = spark.sql(\"\"\"\n",
    "SELECT dt as datetime,\n",
    "    City, \n",
    "    AverageTemperature as average_temperature, \n",
    "    AverageTemperatureUncertainty as average_temperature_uncertainty\n",
    "\n",
    "FROM temp_data\n",
    "\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+-------------------+-------------------------------+\n",
      "|  datetime|   City|average_temperature|average_temperature_uncertainty|\n",
      "+----------+-------+-------------------+-------------------------------+\n",
      "|1820-01-01|Abilene| 2.1010000000000004|                          3.217|\n",
      "|1820-02-01|Abilene|              6.926|                          2.853|\n",
      "|1820-03-01|Abilene|             10.767|                          2.395|\n",
      "|1820-04-01|Abilene| 17.988999999999994|                          2.202|\n",
      "|1820-05-01|Abilene|             21.809|                          2.036|\n",
      "|1820-06-01|Abilene|             25.682|                          2.008|\n",
      "|1820-07-01|Abilene|             26.268|             1.8019999999999998|\n",
      "|1820-08-01|Abilene|             25.048|                          1.895|\n",
      "|1820-09-01|Abilene|             22.435|             2.2159999999999997|\n",
      "|1820-10-01|Abilene|              15.83|                          2.169|\n",
      "+----------+-------+-------------------+-------------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dim_temperature.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Write to parquet files\n",
    "dim_temperature.write.mode(\"overwrite\").parquet(\"dim_temperature\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### Create fact table immigration_case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Create fact table immigration_case\n",
    "fact_immigration_case = spark.sql(\"\"\"\n",
    "SELECT cicid as cic_id,\n",
    "    arrival_date, \n",
    "    i94addr as state_code,\n",
    "    i94mode as immigration_mode, \n",
    "    departure_date, \n",
    "    i94port as airport_code,\n",
    "    residence as residence, \n",
    "    i94visa as visa_code, \n",
    "    visatype as visa_type, \n",
    "    DATEDIFF(departure_date, arrival_date) as duration_of_stay\n",
    "\n",
    "FROM imgr_data\n",
    "\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------+----------+----------------+--------------+------------+--------------------+---------+---------+----------------+\n",
      "|   cic_id|arrival_date|state_code|immigration_mode|departure_date|airport_code|           residence|visa_code|visa_type|duration_of_stay|\n",
      "+---------+------------+----------+----------------+--------------+------------+--------------------+---------+---------+----------------+\n",
      "|4084316.0|  2016-04-22|        HI|             1.0|    2016-04-29|         HHW|               JAPAN|      2.0|       WT|               7|\n",
      "|4422636.0|  2016-04-23|        TX|             1.0|    2016-04-24|         MCA|  MEXICO Air Sea,...|      2.0|       B2|               1|\n",
      "|1195600.0|  2016-04-07|        FL|             1.0|    2016-04-27|         OGG|             GERMANY|      2.0|       WT|              20|\n",
      "|5291768.0|  2016-04-28|        CA|             1.0|    2016-05-07|         LOS|               QATAR|      2.0|       B2|               9|\n",
      "| 985523.0|  2016-04-06|        NY|             3.0|    2016-04-09|         CHM|              FRANCE|      2.0|       WT|               3|\n",
      "|1481650.0|  2016-04-08|        GA|             1.0|    2016-06-01|         ATL|           GUATEMALA|      2.0|       B2|              54|\n",
      "|2197173.0|  2016-04-12|        CA|             1.0|    2016-06-30|         SFR|          CHINA, PRC|      2.0|       B2|              79|\n",
      "| 232708.0|  2016-04-02|        NY|             1.0|    2016-04-10|         NYC|      UNITED KINGDOM|      2.0|       WT|               8|\n",
      "|5227851.0|  2016-04-28|        IL|             1.0|    2016-05-01|         CHI|         SWITZERLAND|      2.0|       WT|               3|\n",
      "|  13213.0|  2016-04-01|        CA|             1.0|    2016-04-09|         LOS|             IRELAND|      2.0|       WT|               8|\n",
      "+---------+------------+----------+----------------+--------------+------------+--------------------+---------+---------+----------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fact_immigration_case.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Write to parquet files\n",
    "fact_immigration_case.write.mode(\"overwrite\").parquet(\"fact_immigration_case\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.2 Data Quality Checks\n",
    "Explain the data quality checks you'll perform to ensure the pipeline ran as expected. These could include:\n",
    " * Integrity constraints on the relational database (e.g., unique key, data type, etc.)\n",
    " * Unit tests for the scripts to ensure they are doing the right thing\n",
    " * Source/Count checks to ensure completeness\n",
    "\n",
    " \n",
    "Run Quality Checks\n",
    "* Get the count of entries for each table \n",
    "* Check each column in each table for null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Perform quality checks here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### Fact immigration table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "951"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fact_immigration_case.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+----------------+----------------+---------+----------+---------+------------+\n",
      "|visa_type|cic_id|duration_of_stay|immigration_mode|residence|state_code|visa_code|airport_code|\n",
      "+---------+------+----------------+----------------+---------+----------+---------+------------+\n",
      "|        0|     0|               0|               0|        0|         0|        0|           0|\n",
      "+---------+------+----------------+----------------+---------+----------+---------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fact_immigration_case.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in list(set(fact_immigration_case.columns) - {'arrival_date','departure_date'})]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### Dimension temperature table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "661524"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dim_temperature.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----+-------------------+-------------------------------+\n",
      "|datetime|City|average_temperature|average_temperature_uncertainty|\n",
      "+--------+----+-------------------+-------------------------------+\n",
      "|       0|   0|                  0|                              0|\n",
      "+--------+----+-------------------+-------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dim_temperature.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in dim_temperature.columns]).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### Dimension airport table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14343"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dim_airport.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------+------------+------------+------------+---------+--------+\n",
      "|airport_code|state_code|airport_type|elevation_ft|airport_name|longitude|latitude|\n",
      "+------------+----------+------------+------------+------------+---------+--------+\n",
      "|           0|         0|           0|           0|           0|        0|       0|\n",
      "+------------+----------+------------+------------+------------+---------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dim_airport.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in dim_airport.columns]).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### Dimension visa table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dim_visa.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------------+\n",
      "|visa_type|visa_description|\n",
      "+---------+----------------+\n",
      "|        0|               0|\n",
      "+---------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dim_visa.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in dim_visa.columns]).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### Dimension state table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dim_state.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+-------+---------+\n",
      "|state_code|state_name|country|continent|\n",
      "+----------+----------+-------+---------+\n",
      "|         0|         0|      0|        0|\n",
      "+----------+----------+-------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dim_state.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in dim_state.columns]).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### Dimension demographics table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "596"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dim_demographics.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+----------------+---------------+-----------------+------------+----------+\n",
      "|city|state|total_population|male_population|female_population|foreign_born|median_age|\n",
      "+----+-----+----------------+---------------+-----------------+------------+----------+\n",
      "|   0|    0|               0|              0|                0|           0|         0|\n",
      "+----+-----+----------------+---------------+-----------------+------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dim_demographics.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in dim_demographics.columns]).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### Dimension immigrant table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "951"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dim_immigrant.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+---------+---+\n",
      "|cic_id|gender|birthyear|age|\n",
      "+------+------+---------+---+\n",
      "|     0|     0|        0|  0|\n",
      "+------+------+---------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dim_immigrant.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in dim_immigrant.columns]).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "No column has entries with null values, so the data quality check passed!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.3 Data dictionary \n",
    "Create a data dictionary for your data model. For each field, provide a brief description of what the data is and where it came from. You can include the data dictionary in the notebook or in a separate file.\n",
    "\n",
    "\n",
    "Sources: \n",
    "* opendatasoft (ods): Demographic data\n",
    "* Kaggle (k): Temperature data\n",
    "* datahub.io (dh): Airport data\n",
    "* National Travel and Tourism Office (NTTO): Immigration data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "<center>dim_airport</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Column name | Description | Source|\n",
    "-------- | -------- | -------- | \n",
    "airport_code   | Code of the airport   |dh  |\n",
    "state_code   | Code of the state the airport is in   |  dh|\n",
    "airport_type   | Type of the airport   | dh|\n",
    "elevation_ft   | Height that the airport lies in ft  | dh|\n",
    "airport_name | Name of the airport|dh|\n",
    "longitude | Coordinate of the airport| dh|\n",
    "latitude | Coordinate of the airport |dh|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "<center>dim_temperature</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Column name | Description |  Source|\n",
    "-------- | -------- | -------- | \n",
    "datetime   | Time the temperature was measured   |k |\n",
    "city   | City that the temperature was measured in   | k|\n",
    "average_temperatur | Average temperature|k|\n",
    "average_temperature_uncertainty | Uncertainty of the measured temperatures|k|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "<center>dim_immigrant</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Column name | Description |  Source|\n",
    "-------- | -------- | -------- | \n",
    "immigrant_id   | Id of the immigrant   | NTTO|\n",
    "gender   | Gender of the immigrant   |NTTO |\n",
    "birthyear | Year that the immigrant is born in|NTTO|\n",
    "age | Age of the immigrant by the time of immigration |NTTO|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "<center>dim_state</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Column name | Description |  Source|\n",
    "-------- | -------- | -------- | \n",
    "state_code   | Code of the state   | ods|\n",
    "state_name   | Name of the state   | ods|\n",
    "continent | Continent that the state is in |dh|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "<center>dim_visa</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Column name | Description |  Source|\n",
    "-------- | -------- | -------- | \n",
    "visa_code   | Code of the visa   | NTTO|\n",
    "visa_description   | Description of the code   |NTTO |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "<center>dim_demographic</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Column name | Description |  Source|\n",
    "-------- | -------- | -------- | \n",
    "city  | Name of the city |ods |\n",
    "state  | Name of the state the city lies in | ods|\n",
    "total_population   | Total population of the city   | ods|\n",
    "male_population | Male population of the city |ods|\n",
    "female_population | Female population of the city|ods|\n",
    "foreign_born | Foreign-born people living in the city |ods|\n",
    "median_age | Median age of the population|ods|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "<center>fact_immigration_case</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Column name | Description |  Source|\n",
    "-------- | -------- | -------- | \n",
    "cicid   | Identifier for the immigration case   |NTTO |\n",
    "arrival_date   | Arrival date of the immigrant   |NTTO|\n",
    "state_code | Code of the state the immigrant came to |NTTO|\n",
    "immigration_mode | Mode that the immigrant came to the US |NTTO|\n",
    "departure_date | Departure date of the immigrant |NTTO|\n",
    "airport_code | Code of the airport the immigrant came to |NTTO|\n",
    "residence | Home of the immigrant |NTTO|\n",
    "visa_code | Code of the visa the immigrant has |NTTO|\n",
    "visa_type | Type of the visa the immigrant has |NTTO|\n",
    "duration_of_stay | Duration of the immigrants stay| Calculated|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Step 5: Complete Project Write Up\n",
    "* Clearly state the rationale for the choice of tools and technologies for the project.\n",
    " * Spark was used as a tool to transform the data because it is made for working with large datasets \n",
    "* Propose how often the data should be updated and why.\n",
    " * The data should be updated monthly, since it relies on the immigration data which is partitioned into months. \n",
    "* Write a description of how you would approach the problem differently under the following scenarios:\n",
    " * The data was increased by 100x.\n",
    "   * If the data was increased by 100x dezentralized methods for the ETL process would be useful. Therefore, the process could be ported to Amazon Reshift.\n",
    " * The data populates a dashboard that must be updated on a daily basis by 7am every day.\n",
    "   * For this purpose, one could use Apache Airflow to create a DAG that runs on a daily schedule. \n",
    " * The database needed to be accessed by 100+ people.\n",
    "   * If the database needed to be accessed by multiple users, a data storage like Amazon S3 would be useful, because it can handle multi party access."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
